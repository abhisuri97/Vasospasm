{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "chicken-touch",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rapid-exhibition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only uncomment if you haven't had these installed before\n",
    "# !pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "official-boxing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-little",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "You should have 4 spreadsheets in the `Data` folder with the following columns:\n",
    "\n",
    "**Data/flowsheets.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* FLO_MEAS_NAME\n",
    "* NORMALIZED_NAME\n",
    "* NFLO_MEAS_ID\n",
    "* GROUPED_FLO_ID\n",
    "* MEAS_VALUE\n",
    "* UNITS\n",
    "* RECODED_TIME\n",
    "\n",
    "\n",
    "**Data/Labs.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* NORMALIZED_LAB_NAME\n",
    "* COMPONENT_ID\n",
    "* COMPONENT_NAME\n",
    "* ORD_VALUE\n",
    "* ORD_NUM_VALUE_CORRECTED\n",
    "* RESULT_TIME\n",
    "\n",
    "\n",
    "**Data/flowsheetspressures.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* FLO_MEAS_NAME\n",
    "* NORMALIZED_NAME\n",
    "* NFLO_MEAS_ID\n",
    "* GROUPED_FLO_ID\n",
    "* MEAS_VALUE\n",
    "* UNITS\n",
    "* RECODED_TIME\n",
    "* SYSTOLIC\n",
    "* DIASTOLIC\n",
    "* MAP\n",
    "\n",
    "\n",
    "**Data/main.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* ADMSN_MINUTES\n",
    "* ICU_MINS\n",
    "* DISCHARGE_MINUTES\n",
    "* IN_ICU_TIME\n",
    "* FIRST_LOCATION\n",
    "* PROC_NAME\n",
    "* EXAM_BEGIN_MINUTES\n",
    "* VERAPAMIL_TAKEN_TIME\n",
    "* CPT_COIL\n",
    "* INPT_DEATH_YN\n",
    "* BMI\n",
    "* AGE_LT90\n",
    "* SEX\n",
    "* ETHNICITY\n",
    "* RACE\n",
    "* ADMITTING_SERVICE\n",
    "* MEANS_OF_ARRIVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bizarre-connectivity",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3427: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "flowsheets = pd.read_csv('Data/flowsheets.csv')\n",
    "labs = pd.read_csv('Data/Labs.csv')\n",
    "flowsheets_pressures = pd.read_csv('Data/flowsheetspressures.csv')\n",
    "pts = pd.read_csv('Data/main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "numerical-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize names of the tests in the flowsheets file\n",
    "mapping_dict = {\n",
    "        'O2 FLOW':'Supplemental O2', \n",
    "        'FIO2':'Supplemental FIO2',\n",
    "        'ETCO2': 'Supplemental ETCO2',\n",
    "        'AIRWAY VIEW GRADE ': 'Mechanical Ventilation',\n",
    "        'AIRWAY DIFFICULTY': 'Mechanical Ventilation',\n",
    "        'ETT NUMBER OF ATTEMPTS': 'Mechanical Ventilation',\n",
    "        'ETT EQUIPMENT': 'Mechanical Ventilation',\n",
    "        'O2 FACE MASK': 'Supplemental O2',\n",
    "        'TUBE OUTPUT': 'TUBE FEEDING',\n",
    "        'MAINTENANCE IV VOLUME': 'IV Fluids',\n",
    "        'O2 NASAL': 'Supplemental O2',\n",
    "        'BLOOD ADMINISTRATION VOLUME': 'Blood Admin',\n",
    "        'BLOOD OUTPUT': 'Blood Loss',\n",
    "        'AIRWAY COMMENTS': 'Mechanical Ventilation',\n",
    "        'GASTRIC TUBE': 'TUBE FEEDING'\n",
    "    }\n",
    "\n",
    "flowsheets.NORMALIZED_NAME.replace(\n",
    "    to_replace=mapping_dict, \n",
    "inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "posted-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_filtered = pts[['HSH_ADMSN_ID', 'DISCHARGE_MINUTES', 'VERAPAMIL_TAKEN_TIME', 'IN_ICU_TIME', 'ICU_MINS', 'BMI', 'AGE_LT90', 'SEX', 'ETHNICITY', 'RACE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "everyday-ozone",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pandas/core/indexing.py:1684: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = infer_fill_value(value)\n",
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "pts_filtered.loc[~pts_filtered['VERAPAMIL_TAKEN_TIME'].isna(), 'VERAPAMIL_TAKEN'] = True\n",
    "pts_filtered.loc[pts_filtered['VERAPAMIL_TAKEN_TIME'].isna(), 'VERAPAMIL_TAKEN'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fitting-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "pts_filtered['VERAPAMIL_TAKEN_TIME'] = pts_filtered['VERAPAMIL_TAKEN_TIME'].fillna(pts_filtered['ICU_MINS'] + pts_filtered['IN_ICU_TIME'])\n",
    "pts_filtered['VERAPAMIL_TAKEN_TIME'] = pd.to_numeric(pts_filtered['VERAPAMIL_TAKEN_TIME'])\n",
    "\n",
    "cutpts = [240, 1440, 2880, 4320, 5760, 7200, 10080, 14400, 20160]\n",
    "\n",
    "for t in cutpts:\n",
    "    pts_filtered[f\"{t}_cutpoint\"] = pts_filtered['IN_ICU_TIME'] + t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "statewide-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowsheets = flowsheets[['HSH_ADMSN_ID', 'NORMALIZED_NAME', 'MEAS_VALUE', 'RECODED_TIME']]\n",
    "flowsheets_pressures = flowsheets_pressures[['HSH_ADMSN_ID', 'NORMALIZED_NAME', 'SYSTOLIC', 'DIASTOLIC', 'MAP', 'RECODED_TIME']]\n",
    "countlabs = labs.groupby('HSH_ADMSN_ID').agg('count')\n",
    "lab_filtered = countlabs[countlabs['NORMALIZED_LAB_NAME'] > 0].reset_index().rename(\n",
    "    columns={'NORMALIZED_LAB_NAME': 'lab_count'})[['HSH_ADMSN_ID', 'lab_count']]\n",
    "\n",
    "labs_clean = labs.merge(lab_filtered, on='HSH_ADMSN_ID', how='right')[['HSH_ADMSN_ID','NORMALIZED_LAB_NAME', 'ORD_NUM_VALUE_CORRECTED', 'RESULT_TIME']]\n",
    "\n",
    "cutpts_nms = [f\"{x}_cutpoint\" for x in cutpts]\n",
    "cutpoints = pts_filtered[['HSH_ADMSN_ID'] + cutpts_nms]\n",
    "\n",
    "flowsheets_w_cutpoints = flowsheets.merge(cutpoints, on='HSH_ADMSN_ID', how='left')\n",
    "flowsheets_pressures_w_cutpoints = flowsheets_pressures.merge(cutpoints, on='HSH_ADMSN_ID', how='left')\n",
    "labs_clean_w_cutpoints = labs_clean.merge(cutpoints, on='HSH_ADMSN_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unavailable-timeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def rename(newname):\n",
    "    def decorator(f):\n",
    "        f.__name__ = newname\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "def q_at(y):\n",
    "    @rename(f'q{y:0.2f}')\n",
    "    def q(x):\n",
    "        return x.quantile(y)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "olympic-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_full_sheet(T):    \n",
    "\n",
    "    print(\"process lab data\")\n",
    "    l = labs_clean_w_cutpoints.copy()\n",
    "\n",
    "    l_time = l[l['RESULT_TIME'] < l[f'{T}_cutpoint']] #only get labs before cutpoint time\n",
    "\n",
    "    f = {'ORD_NUM_VALUE_CORRECTED': ['min', 'max', 'mean', 'count']} # setup for aggregation of labs. didn't do 21 feat vector...too long\n",
    "\n",
    "    lab_quantiles = l_time.groupby(['HSH_ADMSN_ID', 'NORMALIZED_LAB_NAME']).agg(f) # gets min, max, mean, and count for each lab\n",
    "\n",
    "    lab_quantiles.columns = lab_quantiles.columns.to_flat_index() # flatten to prep for pivot\n",
    "\n",
    "    lab_quantiles_flat = lab_quantiles.reset_index() # reset index to prep for pivot\n",
    "\n",
    "    # pivot labs to have table structure with \n",
    "    #       LAB1 min, LAB1 max, LAB1 mean, LAB1 count, LAB2 min ...\n",
    "    # ptID  ...\n",
    "\n",
    "    wide_labs = lab_quantiles_flat.pivot(index='HSH_ADMSN_ID', columns='NORMALIZED_LAB_NAME') \n",
    "\n",
    "    wide_labs.columns = wide_labs.columns.to_flat_index()\n",
    "\n",
    "    print(\"process ICP data\")\n",
    "\n",
    "    icp = flowsheets_w_cutpoints.copy()\n",
    "\n",
    "    icp_time = icp[icp['RECODED_TIME'] < icp[f'{T}_cutpoint']] # only get icp before cutpoint time\n",
    "\n",
    "    icp = icp[icp['NORMALIZED_NAME'] == 'ICP'] # only care about ICP data in this \n",
    " \n",
    "    icp['ICP_MEAS_VALUE'] = pd.to_numeric(icp['MEAS_VALUE']) # some non numbers, just coerce to 0\n",
    "\n",
    "    f = {'ICP_MEAS_VALUE': [q_at(x) for x in np.linspace(0.05,1,20)]+['count']} # grouping stuff to get the 20 percentiles\n",
    "\n",
    "    icp_quantiles = icp.groupby(['HSH_ADMSN_ID']).agg(f) \n",
    "\n",
    "    icp_quantiles.columns = icp_quantiles.columns.to_flat_index() \n",
    "\n",
    "    icp_quantiles_no_lt100 = icp_quantiles \n",
    "\n",
    "    print(\"process BP data\")\n",
    "\n",
    "    bp = flowsheets_pressures_w_cutpoints.copy() \n",
    "\n",
    "    bp = bp[bp['RECODED_TIME'] < bp[f'{T}_cutpoint']] # only stuff before the cutpoint\n",
    "\n",
    "    f = {'MAP': [q_at(x) for x in np.linspace(0.05,1,20)]+['count']} # same 20 percentile thing thing\n",
    "\n",
    "    bp_quantiles = bp.groupby(['HSH_ADMSN_ID','NORMALIZED_NAME']).agg(f)\n",
    "\n",
    "    bp_quantiles.columns = bp_quantiles.columns.to_flat_index()\n",
    "\n",
    "    bp_quantiles_flat = bp_quantiles.reset_index()\n",
    "\n",
    "    wide_bp = bp_quantiles_flat.pivot(index='HSH_ADMSN_ID', columns='NORMALIZED_NAME')\n",
    "\n",
    "    wide_bp.columns = wide_bp.columns.to_flat_index()\n",
    "\n",
    "    print(\"get the counts for the variables etc.\")\n",
    "\n",
    "    icp_analysis_one = icp_time.groupby(['HSH_ADMSN_ID', 'NORMALIZED_NAME']).count()[['MEAS_VALUE']]\n",
    "\n",
    "    icp_analysis_one_raw = icp_analysis_one.reset_index().pivot(index='HSH_ADMSN_ID', columns='NORMALIZED_NAME')\n",
    "\n",
    "    icp_analysis_one_raw.columns = icp_analysis_one_raw.columns.to_flat_index()\n",
    "\n",
    "    icp_analysis_bool = icp_analysis_one_raw.notnull()\n",
    "\n",
    "    icp_analysis_count = icp_analysis_one_raw.fillna(0)\n",
    "\n",
    "    demos = pts_filtered[['HSH_ADMSN_ID', 'VERAPAMIL_TAKEN_TIME', 'VERAPAMIL_TAKEN', f'{T}_cutpoint', 'BMI', 'AGE_LT90', 'SEX', 'ETHNICITY', 'RACE']]\n",
    "\n",
    "    demos_labeled = demos[['HSH_ADMSN_ID', 'VERAPAMIL_TAKEN', f'{T}_cutpoint', 'VERAPAMIL_TAKEN_TIME', 'BMI', 'AGE_LT90', 'SEX', 'ETHNICITY', 'RACE']]\n",
    "    \"\"\"\n",
    "    variables:\n",
    "    1. mixed vars binary = icp_analysis_bool\n",
    "    2. mixed vars counts = icp_analysis_count\n",
    "    3. ICP actual values = icp_quantiles_no_lt100\n",
    "    4. BP = wide_bp\n",
    "    5. lab = wide_labs (use this as the main index probs)\n",
    "    6. demographics = demos_labeled\n",
    "    \"\"\"\n",
    "    print(\"merging..\")\n",
    "    final_df = wide_labs.reset_index().merge(demos_labeled, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(wide_bp, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(icp_quantiles_no_lt100, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(icp_analysis_count, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(icp_analysis_bool, how='left', on='HSH_ADMSN_ID')\n",
    "    \n",
    "    final_df = final_df[final_df['VERAPAMIL_TAKEN_TIME'] > final_df[f'{T}_cutpoint']]\n",
    "    def relabel_verap(row):\n",
    "        if row['VERAPAMIL_TAKEN'] == False:\n",
    "            return \"False\"\n",
    "        else:\n",
    "            # return \"True\"\n",
    "            res = int((row['VERAPAMIL_TAKEN_TIME']-row[f'{T}_cutpoint'])/1440)\n",
    "            if res < 3:\n",
    "                return f\"Will need verapamil in [0-3) days\"\n",
    "            else:\n",
    "                return f\"Will need verapamil in 3+ days\"\n",
    "    final_df['VERAPAMIL_TAKEN'] = final_df.apply(relabel_verap, axis=1)\n",
    "    print(final_df['VERAPAMIL_TAKEN'].value_counts())\n",
    "    final_df = final_df.drop(columns=[f'{T}_cutpoint', 'VERAPAMIL_TAKEN_TIME'])\n",
    "    print(f\"saving to Cleaned-timeto/{T}.csv\")\n",
    "    final_df.to_csv(f'Cleaned-timeto/{T}.csv')\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "executed-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making for t= 240\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1901\n",
      "False                                1762\n",
      "Will need verapamil in 3+ days        100\n",
      "Will need verapamil in [0-3) days      24\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/240.csv\n",
      "==Value Counts==\n",
      "False                                1762\n",
      "Will need verapamil in 3+ days        100\n",
      "Will need verapamil in [0-3) days      24\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n",
      "Making for t= 1440\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1947\n",
      "False                                1555\n",
      "Will need verapamil in 3+ days         99\n",
      "Will need verapamil in [0-3) days      16\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/1440.csv\n",
      "==Value Counts==\n",
      "False                                1555\n",
      "Will need verapamil in 3+ days         99\n",
      "Will need verapamil in [0-3) days      16\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n",
      "Making for t= 2880\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1947\n",
      "False                                1217\n",
      "Will need verapamil in 3+ days         84\n",
      "Will need verapamil in [0-3) days      26\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/2880.csv\n",
      "==Value Counts==\n",
      "False                                1217\n",
      "Will need verapamil in 3+ days         84\n",
      "Will need verapamil in [0-3) days      26\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n",
      "Making for t= 4320\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1947\n",
      "False                                1009\n",
      "Will need verapamil in 3+ days         72\n",
      "Will need verapamil in [0-3) days      32\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/4320.csv\n",
      "==Value Counts==\n",
      "False                                1009\n",
      "Will need verapamil in 3+ days         72\n",
      "Will need verapamil in [0-3) days      32\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n",
      "Making for t= 7200\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1948\n",
      "False                                782\n",
      "Will need verapamil in 3+ days        51\n",
      "Will need verapamil in [0-3) days     33\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/7200.csv\n",
      "==Value Counts==\n",
      "False                                782\n",
      "Will need verapamil in 3+ days        51\n",
      "Will need verapamil in [0-3) days     33\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n",
      "Making for t= 10080\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1948\n",
      "False                                670\n",
      "Will need verapamil in 3+ days        32\n",
      "Will need verapamil in [0-3) days     28\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/10080.csv\n",
      "==Value Counts==\n",
      "False                                670\n",
      "Will need verapamil in 3+ days        32\n",
      "Will need verapamil in [0-3) days     28\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n",
      "Making for t= 14400\n",
      "process lab data\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "1948\n",
      "False                                527\n",
      "Will need verapamil in [0-3) days     17\n",
      "Will need verapamil in 3+ days        15\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "saving to Cleaned-timeto/14400.csv\n",
      "==Value Counts==\n",
      "False                                527\n",
      "Will need verapamil in [0-3) days     17\n",
      "Will need verapamil in 3+ days        15\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_tmpts = []\n",
    "for t in [240, 1440, 2880, 4320, 7200, 10080, 14400]:\n",
    "    print(\"Making for t=\", t)\n",
    "    df_tmpt = output_full_sheet(t)\n",
    "    df_tmpts.append(df_tmpt)\n",
    "    print(\"==Value Counts==\")\n",
    "    print(df_tmpt['VERAPAMIL_TAKEN'].value_counts())\n",
    "    print(\"====\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "elegant-bahrain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for t in [240, 1440, 2880, 4320, 7200, 10080, 14400]:\n",
    "    predictors = pd.read_csv('annotated_predictors.csv')\n",
    "    inc_columns = predictors[predictors['physiologic'] == 1.0]['HSH_ADMSN_ID'].to_numpy()\n",
    "    min_columns = filter(lambda x: x.find('count') == -1 and x.find('_x') == -1, inc_columns)\n",
    "    csv_orig = pd.read_csv(f'Cleaned-timeto/{t}.csv')\n",
    "    csv_orig[csv_orig.columns & (['HSH_ADMSN_ID', 'VERAPAMIL_TAKEN'] + list(min_columns))].to_csv(f'Cleaned-timeto/{t}_nocounts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

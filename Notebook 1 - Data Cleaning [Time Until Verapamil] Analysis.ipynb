{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "changing-broadcasting",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "textile-trance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only uncomment if you haven't had these installed before\n",
    "# !pip install numpy pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bacterial-helicopter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-reflection",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-scheme",
   "metadata": {},
   "source": [
    "You should have 4 spreadsheets in the `Data` folder with the following columns:\n",
    "\n",
    "**Data/flowsheets.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* FLO_MEAS_NAME\n",
    "* NORMALIZED_NAME\n",
    "* NFLO_MEAS_ID\n",
    "* GROUPED_FLO_ID\n",
    "* MEAS_VALUE\n",
    "* UNITS\n",
    "* RECODED_TIME\n",
    "\n",
    "\n",
    "**Data/Labs.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* NORMALIZED_LAB_NAME\n",
    "* COMPONENT_ID\n",
    "* COMPONENT_NAME\n",
    "* ORD_VALUE\n",
    "* ORD_NUM_VALUE_CORRECTED\n",
    "* RESULT_TIME\n",
    "\n",
    "\n",
    "**Data/flowsheetspressures.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* FLO_MEAS_NAME\n",
    "* NORMALIZED_NAME\n",
    "* NFLO_MEAS_ID\n",
    "* GROUPED_FLO_ID\n",
    "* MEAS_VALUE\n",
    "* UNITS\n",
    "* RECODED_TIME\n",
    "* SYSTOLIC\n",
    "* DIASTOLIC\n",
    "* MAP\n",
    "\n",
    "\n",
    "**Data/main.csv**\n",
    "* HSH_ADMSN_ID\n",
    "* ADMSN_MINUTES\n",
    "* ICU_MINS\n",
    "* DISCHARGE_MINUTES\n",
    "* IN_ICU_TIME\n",
    "* FIRST_LOCATION\n",
    "* PROC_NAME\n",
    "* EXAM_BEGIN_MINUTES\n",
    "* VERAPAMIL_TAKEN_TIME\n",
    "* CPT_COIL\n",
    "* INPT_DEATH_YN\n",
    "* BMI\n",
    "* AGE_LT90\n",
    "* SEX\n",
    "* ETHNICITY\n",
    "* RACE\n",
    "* ADMITTING_SERVICE\n",
    "* MEANS_OF_ARRIVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hybrid-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3427: DtypeWarning: Columns (5) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "flowsheets = pd.read_csv('Data/flowsheets.csv')\n",
    "labs = pd.read_csv('Data/Labs.csv')\n",
    "flowsheets_pressures = pd.read_csv('Data/flowsheetspressures.csv')\n",
    "pts = pd.read_csv('Data/main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opposite-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize names of the tests in the flowsheets file\n",
    "mapping_dict = {\n",
    "        'O2 FLOW':'Supplemental O2', \n",
    "        'FIO2':'Supplemental FIO2',\n",
    "        'ETCO2': 'Supplemental ETCO2',\n",
    "        'AIRWAY VIEW GRADE ': 'Mechanical Ventilation',\n",
    "        'AIRWAY DIFFICULTY': 'Mechanical Ventilation',\n",
    "        'ETT NUMBER OF ATTEMPTS': 'Mechanical Ventilation',\n",
    "        'ETT EQUIPMENT': 'Mechanical Ventilation',\n",
    "        'O2 FACE MASK': 'Supplemental O2',\n",
    "        'TUBE OUTPUT': 'TUBE FEEDING',\n",
    "        'MAINTENANCE IV VOLUME': 'IV Fluids',\n",
    "        'O2 NASAL': 'Supplemental O2',\n",
    "        'BLOOD ADMINISTRATION VOLUME': 'Blood Admin',\n",
    "        'BLOOD OUTPUT': 'Blood Loss',\n",
    "        'AIRWAY COMMENTS': 'Mechanical Ventilation',\n",
    "        'GASTRIC TUBE': 'TUBE FEEDING'\n",
    "    }\n",
    "\n",
    "flowsheets.NORMALIZED_NAME.replace(\n",
    "    to_replace=mapping_dict, \n",
    "inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-skirt",
   "metadata": {},
   "source": [
    "# Filter step 1: \n",
    "* This next set of code will solated the needed columns and establish time cutpoints for which the algorithm considers data until."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "noticed-determination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Filtering data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepting-vermont",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pandas/core/indexing.py:1684: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = infer_fill_value(value)\n",
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    }
   ],
   "source": [
    "pts_filtered = pts[['HSH_ADMSN_ID', 'DISCHARGE_MINUTES', 'VERAPAMIL_TAKEN_TIME', \n",
    "                    'IN_ICU_TIME', 'ICU_MINS', 'BMI', 'AGE_LT90', \n",
    "                    'SEX', 'ETHNICITY', 'RACE']]\n",
    "# Relabel patients as taken vs not taken VERAPAMIL\n",
    "pts_filtered.loc[~pts_filtered['VERAPAMIL_TAKEN_TIME'].isna(), 'VERAPAMIL_TAKEN'] = True\n",
    "pts_filtered.loc[pts_filtered['VERAPAMIL_TAKEN_TIME'].isna(), 'VERAPAMIL_TAKEN'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "artificial-behavior",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pts_filtered['VERAPAMIL_TAKEN_TIME'] = pts_filtered['VERAPAMIL_TAKEN_TIME'].fillna(pts_filtered['ICU_MINS'] + pts_filtered['IN_ICU_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "retained-postage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "pts_filtered['VERAPAMIL_TAKEN_TIME'] = pd.to_numeric(pts_filtered['VERAPAMIL_TAKEN_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aerial-dating",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for t in [240, 1440, 2880, 4320, 7200, 10080, 14400, 20160]:\n",
    "    pts_filtered[f\"{t}_cutpoint\"] = pts_filtered['VERAPAMIL_TAKEN_TIME'] - t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-variable",
   "metadata": {},
   "source": [
    "# Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inclusive-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hidden-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowsheets = flowsheets[['HSH_ADMSN_ID', 'NORMALIZED_NAME', 'MEAS_VALUE', 'RECODED_TIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "featured-string",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowsheets_pressures = flowsheets_pressures[['HSH_ADMSN_ID', 'NORMALIZED_NAME', 'SYSTOLIC', 'DIASTOLIC', 'MAP', 'RECODED_TIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abstract-output",
   "metadata": {},
   "outputs": [],
   "source": [
    "countlabs = labs.groupby('HSH_ADMSN_ID').agg('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dangerous-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out labs with no data\n",
    "lab_filtered = countlabs[countlabs['NORMALIZED_LAB_NAME'] > 0].reset_index().rename(\n",
    "    columns={'NORMALIZED_LAB_NAME': 'lab_count'})[['HSH_ADMSN_ID', 'lab_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "vanilla-desktop",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_clean = labs.merge(lab_filtered, on='HSH_ADMSN_ID', how='right')[['HSH_ADMSN_ID','NORMALIZED_LAB_NAME', 'ORD_NUM_VALUE_CORRECTED', 'RESULT_TIME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "parental-uruguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutpoints = pts_filtered[['HSH_ADMSN_ID', '240_cutpoint', '1440_cutpoint', '2880_cutpoint', '4320_cutpoint', \n",
    "                          '7200_cutpoint', '10080_cutpoint', '14400_cutpoint', '20160_cutpoint']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "catholic-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowsheets_w_cutpoints = flowsheets.merge(cutpoints, on='HSH_ADMSN_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "amazing-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowsheets_pressures_w_cutpoints = flowsheets_pressures.merge(cutpoints, on='HSH_ADMSN_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "residential-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_clean_w_cutpoints = labs_clean.merge(cutpoints, on='HSH_ADMSN_ID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "latter-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper\n",
    "def rename(newname):\n",
    "    def decorator(f):\n",
    "        f.__name__ = newname\n",
    "        return f\n",
    "    return decorator\n",
    "\n",
    "def q_at(y):\n",
    "    @rename(f'q{y:0.2f}')\n",
    "    def q(x):\n",
    "        return x.quantile(y)\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "binding-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_full_sheet(T):    \n",
    "\n",
    "    print(\"Processing Data for t =\", T, \"minutes\")\n",
    "    l = labs_clean_w_cutpoints.copy()\n",
    "\n",
    "    l_time = l[l['RESULT_TIME'] < l[f'{T}_cutpoint']] #only get labs before cutpoint time\n",
    "\n",
    "    f = {'ORD_NUM_VALUE_CORRECTED': ['min', 'max', 'mean', 'count']} # setup for aggregation of labs. didn't do 21 feat vector...too long\n",
    "\n",
    "    lab_quantiles = l_time.groupby(['HSH_ADMSN_ID', 'NORMALIZED_LAB_NAME']).agg(f) # gets min, max, mean, and count for each lab\n",
    "\n",
    "    lab_quantiles.columns = lab_quantiles.columns.to_flat_index() # flatten to prep for pivot\n",
    "\n",
    "    lab_quantiles_flat = lab_quantiles.reset_index() # reset index to prep for pivot\n",
    "\n",
    "    wide_labs = lab_quantiles_flat.pivot(index='HSH_ADMSN_ID', columns='NORMALIZED_LAB_NAME') \n",
    "\n",
    "    wide_labs.columns = wide_labs.columns.to_flat_index()\n",
    "\n",
    "    print(\"process ICP data\")\n",
    "\n",
    "    icp = flowsheets_w_cutpoints.copy()\n",
    "\n",
    "    icp_time = icp[icp['RECODED_TIME'] < icp[f'{T}_cutpoint']] # only get icp before cutpoint time\n",
    "\n",
    "    icp = icp[icp['NORMALIZED_NAME'] == 'ICP'] # only care about ICP data in this \n",
    " \n",
    "    icp['ICP_MEAS_VALUE'] = pd.to_numeric(icp['MEAS_VALUE']) # some non numbers, just coerce to 0\n",
    "\n",
    "    f = {'ICP_MEAS_VALUE': [q_at(x) for x in np.linspace(0.05,1,20)]+['count']} # grouping stuff to get the 20 percentiles\n",
    "\n",
    "    icp_quantiles = icp.groupby(['HSH_ADMSN_ID']).agg(f) \n",
    "\n",
    "    icp_quantiles.columns = icp_quantiles.columns.to_flat_index() \n",
    "\n",
    "    icp_quantiles_no_lt100 = icp_quantiles \n",
    "\n",
    "    print(\"process BP data\")\n",
    "\n",
    "    bp = flowsheets_pressures_w_cutpoints.copy() \n",
    "\n",
    "    bp = bp[bp['RECODED_TIME'] < bp[f'{T}_cutpoint']] # only stuff after the cutpoint\n",
    "\n",
    "    f = {'MAP': [q_at(x) for x in np.linspace(0.05,1,20)]+['count']} # same 20 percentile thing thing\n",
    "\n",
    "    bp_quantiles = bp.groupby(['HSH_ADMSN_ID','NORMALIZED_NAME']).agg(f)\n",
    "\n",
    "    bp_quantiles.columns = bp_quantiles.columns.to_flat_index()\n",
    "\n",
    "    bp_quantiles_flat = bp_quantiles.reset_index()\n",
    "\n",
    "    wide_bp = bp_quantiles_flat.pivot(index='HSH_ADMSN_ID', columns='NORMALIZED_NAME')\n",
    "\n",
    "    wide_bp.columns = wide_bp.columns.to_flat_index()\n",
    "\n",
    "    print(\"get the counts for the variables etc.\")\n",
    "\n",
    "    icp_analysis_one = icp_time.groupby(['HSH_ADMSN_ID', 'NORMALIZED_NAME']).count()[['MEAS_VALUE']]\n",
    "\n",
    "    icp_analysis_one_raw = icp_analysis_one.reset_index().pivot(index='HSH_ADMSN_ID', columns='NORMALIZED_NAME')\n",
    "\n",
    "    icp_analysis_one_raw.columns = icp_analysis_one_raw.columns.to_flat_index()\n",
    "\n",
    "    icp_analysis_bool = icp_analysis_one_raw.notnull()\n",
    "\n",
    "    icp_analysis_count = icp_analysis_one_raw.fillna(0)\n",
    "\n",
    "    print(\"take care of demographics and finally code if someone got varap or not\")\n",
    "\n",
    "    demos = pts_filtered[['HSH_ADMSN_ID', 'VERAPAMIL_TAKEN_TIME', 'VERAPAMIL_TAKEN', 'BMI', 'AGE_LT90', 'SEX', 'ETHNICITY', 'RACE']]\n",
    "\n",
    "    # demos.loc[demos['VERAPAMIL_TAKEN_TIME'] == 99999999.0, 'VERAPAMIL_TAKEN'] = False\n",
    "\n",
    "    # demos.loc[demos['VERAPAMIL_TAKEN_TIME'] < 99999999, 'VERAPAMIL_TAKEN'] = True\n",
    "\n",
    "    demos_labeled = demos[['HSH_ADMSN_ID', 'VERAPAMIL_TAKEN', 'BMI', 'AGE_LT90', 'SEX', 'ETHNICITY', 'RACE']]\n",
    "    \n",
    "    \"\"\"\n",
    "    variables:\n",
    "    1. mixed vars binary = icp_analysis_bool\n",
    "    2. mixed vars counts = icp_analysis_count\n",
    "    3. ICP actual values = icp_quantiles_no_lt100\n",
    "    4. BP = wide_bp\n",
    "    5. lab = wide_labs (use this as the main index probs)\n",
    "    6. demographics = demos_labeled\n",
    "    \"\"\"\n",
    "    print(\"merging..\")\n",
    "    final_df = wide_labs.reset_index().merge(demos_labeled, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(wide_bp, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(icp_quantiles_no_lt100, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(icp_analysis_count, how='left', on='HSH_ADMSN_ID')\\\n",
    "        .merge(icp_analysis_bool, how='left', on='HSH_ADMSN_ID')\n",
    "    print(f\"saving to Cleaned/{T}.csv\")\n",
    "    final_df.to_csv(f'Cleaned/{T}.csv')\n",
    "    print(\"==Value counts==\")\n",
    "    print(final_df['VERAPAMIL_TAKEN'].value_counts())\n",
    "    print(\"====\")\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "instrumental-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 240 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/240.csv\n",
      "==Value counts==\n",
      "False    1817\n",
      "True      125\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_240 = output_full_sheet(240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "undefined-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 1440 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/1440.csv\n",
      "==Value counts==\n",
      "False    1624\n",
      "True      118\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_1440 = output_full_sheet(1440)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "banner-johnson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 2880 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/2880.csv\n",
      "==Value counts==\n",
      "False    1282\n",
      "True      111\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_2880 = output_full_sheet(2880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dynamic-comfort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 4320 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/4320.csv\n",
      "==Value counts==\n",
      "False    1059\n",
      "True      103\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_4320 = output_full_sheet(4320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "marine-mongolia",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 7200 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/7200.csv\n",
      "==Value counts==\n",
      "False    828\n",
      "True      85\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_7200 = output_full_sheet(7200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adapted-jaguar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 10080 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/10080.csv\n",
      "==Value counts==\n",
      "False    713\n",
      "True      63\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_10080 = output_full_sheet(10080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "former-subscription",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 14400 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/14400.csv\n",
      "==Value counts==\n",
      "False    561\n",
      "True      34\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_14400 = output_full_sheet(14400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "employed-monkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Data for t = 20160 minutes\n",
      "process ICP data\n",
      "process BP data\n",
      "get the counts for the variables etc.\n",
      "take care of demographics and finally code if someone got varap or not\n",
      "merging..\n",
      "saving to Cleaned/20160.csv\n",
      "==Value counts==\n",
      "False    425\n",
      "True      12\n",
      "Name: VERAPAMIL_TAKEN, dtype: int64\n",
      "====\n"
     ]
    }
   ],
   "source": [
    "df_20160 = output_full_sheet(20160)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-insured",
   "metadata": {},
   "source": [
    "## Adjust csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efficient-geology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhi/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: Index.__and__ operating as a set operation is deprecated, in the future this will be a logical operation matching Series.__and__.  Use index.intersection(other) instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in [240, 1440, 2880, 4320, 7200, 10080, 14400, 20160]:\n",
    "    predictors = pd.read_csv('annotated_predictors.csv')\n",
    "    inc_columns = predictors[predictors['physiologic'] == 1.0]['HSH_ADMSN_ID'].to_numpy()\n",
    "    min_columns = filter(lambda x: x.find('count') == -1 and x.find('_x') == -1, inc_columns)\n",
    "    csv_orig = pd.read_csv(f'Cleaned/{t}.csv')\n",
    "    csv_orig[csv_orig.columns & (['HSH_ADMSN_ID', 'VERAPAMIL_TAKEN'] + list(min_columns))].to_csv(f'Cleaned/{t}_nocounts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-voice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
